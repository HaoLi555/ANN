########################
# Missing Files
########################
# .DS_Store

########################
# Additional Files
########################
# choose_sentence.py
# README.md
# tokenizer
# run.sh
# output.txt

########################
# Filled Code
########################
# ../codes/main.py:1
            tgt_ids = input_ids[:,[i for i in range(1,input_ids.shape[1])]+[0]]
            loss_mask = torch.where(tgt_ids==PAD_ID,0,1)
            for seq_ind in range(0,loss_mask.shape[0]):
                for tok_ind in range(0,loss_mask.shape[1]):
                    if loss_mask[seq_ind][tok_ind]==0:
                        loss_mask[seq_ind][tok_ind]=1
                        break
            loss = torch.reshape(loss,lm_logits.shape[:-1])*loss_mask
            loss=torch.sum(loss,1)/torch.sum(loss_mask,dim=1)

# ../codes/model_tfmr.py:1
            torch.tril(torch.ones(max_positions,max_positions,dtype=bool)).repeat(1,1,1,1)

# ../codes/model_tfmr.py:2
        # (batch_size, num_attn_heads, sequence_length, query_num)
        attn_weights = torch.matmul(key,torch.transpose(query,2,3))
        seq_len=attn_weights.shape[2]
        query_num=attn_weights.shape[3]
        causal_mask = self.bias[:,:,seq_len-query_num:seq_len,:seq_len]

        # 2 transposes: for alignment in params with the gpt2 model which the pretrained model params were generated
        attn_weights = torch.transpose(attn_weights,2,3)
        attn_weights = torch.transpose(attn_weights,2 ,3)
        attn_weights = torch.softmax(attn_weights,dim=2)
        # (batch_size, num_attn_heads, attn_head_size, sequence_length)
        attn_output = torch.matmul(torch.transpose(value,2,3),attn_weights)
        # (batch_size, num_attn_heads, sequence_length, attn_head_size)
        attn_output=torch.transpose(attn_output,2,3)
        # output (batch_size, num_attn_heads, sequence_length=softmax, sequence_length=query_num)
        # weights (batch_size, num_attn_heads, sequence_length=softmax, sequence_length=query_num)

# ../codes/model_tfmr.py:3
        temp=torch.split(tensor,attn_head_size,2)
        return torch.stack(temp,dim=1)

# ../codes/model_tfmr.py:4
        return torch.squeeze(torch.cat(torch.split(tensor,1,dim=1),dim=3),dim=1)

# ../codes/model_tfmr.py:5
        # HINT: You can refer to Page 39 in lecture 8 for more details
        hidden_states = attn_output+residual

        residual=hidden_states
        hidden_states=self.ln_2(hidden_states)
        hidden_states=self.mlp(hidden_states)
        hidden_states=residual+hidden_states

# ../codes/model_tfmr.py:6
        position_embeds = self.wpe(torch.arange(past_length,past_length+input_shape[-1],device=device)).repeat(input_shape[0],1,1)

# ../codes/model_tfmr.py:7
            golden_labels=labels[:,[i for i in range(1,labels.shape[1])]+[0]]
            ce=ce_loss_fct(input=torch.reshape(lm_logits,(-1,lm_logits.shape[-1])),target=torch.reshape(golden_labels,(-1,)))
            ce=torch.reshape(ce,labels.shape)

            padded=torch.where(golden_labels==PAD_ID,0,1)
            for seq_index in range(0,padded.shape[0]):
                for tok_index in range(0,padded.shape[1]):
                    if padded[seq_index][tok_index]==0:
                        padded[seq_index][tok_index]=1
                        break

            ce=ce*padded

            temp=torch.sum(ce,dim=1)/torch.sum(padded,dim=1)
            loss=temp.sum()/temp.shape[0]

# ../codes/model_tfmr.py:8
                        prob = logits.softmax(dim=-1)
                        prob, indices=torch.sort(prob,descending=False,dim=-1)

                        mask=torch.cumsum(prob,dim=-1)>(1-top_p)
                        prob=prob*mask
                        sampled=torch.multinomial(prob,1) # (batch_size, 1)
                        now_token=torch.gather(indices, 1, sampled)


########################
# References
########################

########################
# Other Modifications
########################
# _codes/main.py -> ../codes/main.py
# 14 + import wandb
# 37 - parser.add_argument("--train_dir", type=str, default="./train_test",
# 37 ?                                                              -----
# 38 + parser.add_argument("--train_dir", type=str, default="./train",
# 39 - parser.add_argument("--pretrain_dir", type=str, default="None",
# 39 ?                                                         -    -
# 40 + parser.add_argument("--pretrain_dir", type=str, default=None,
# 107 +
# 101 -         weights = np.ones(ngrams) / ngrams
# 108 +         # weights = np.ones(ngrams) / ngrams
# 108 ?        ++
# 109 +         # modified because for a np.ndarray, weights[0][0] raises IndexError while for a list, weights[0][0] raises TypeError
# 110 +         weights = [1/ngrams] * ngrams
# 150 - def load_model(pretrained_dir, model_name="pretrained_ckpt.bin"):
# 159 + def load_model(pretrained_dir, model_name="pretrained_ckpt.bin", config_file="config.json"):
# 159 ?                                                                +++++++++++++++++++++++++++
# 155 -         config_path = os.path.join(pretrained_dir, "config.json")
# 155 ?                                                    -      ^^^^^^
# 164 +         config_path = os.path.join(pretrained_dir, config_file)
# 164 ?                                                          ^^^^^
# 198 +
# 199 +     wandb.init(
# 200 +         project='ANN-HW3',
# 201 +         name=args.name if args.test is None else args.test+"_"+args.decode_strategy+"_"+str(args.temperature),
# 202 +         config=vars(args)
# 203 +     )
# 204 +
# 244 -                 with open(os.path.join(args.train_dir, "config.json"), "w") as f:
# 260 +                 with open(os.path.join(args.train_dir, f"config_{args.name}.json"), "w") as f:
# 260 ?                                                        +       ++++++++++++
# 270 +
# 271 +                 wandb.log({
# 272 +                     "training loss":train_loss,
# 273 +                     "validation loss":val_loss,
# 274 +                     "validation perplexity":val_ppl
# 275 +                 },step=epoch)
# 259 -         model, config = load_model(args.train_dir, model_name=f"checkpoint_{args.test}.bin")
# 281 +         model, config = load_model(args.train_dir, model_name=f"checkpoint_{args.test}.bin",config_file=f"config_{args.test}.json")
# 281 ?                                                                                            +++++++++++++++++++++++++++++++++++++++
# 266 -         with open(f"output_{args.decode_strategy}.txt", "w") as fout:
# 288 +         with open(f"output_{args.test}_{args.decode_strategy}_{str(args.temperature)}.txt", "w") as fout:
# 288 ?                                  ++++++++++++               ++++++++++++++++++++++++
# 273 -         print(f"        test_set, write inference results to output_{args.decode_strategy}.txt")
# 295 +         print(f"        test_set, write inference results to output_{args.name}_{args.decode_strategy}_{str(args.temperature)}.txt")
# 295 ?                                                                           ++++++++++++                ++++++++++++++++++++++++
# 297 +     wandb.finish()
# _codes/model_tfmr.py -> ../codes/model_tfmr.py
# 6 + from tqdm import tqdm
# 306 -             for i in range(0, int(5000/batch_size)+1):
# 342 +             for i in tqdm(range(0, int(5000/batch_size)+1),desc="batch"):
# 342 ?                      +++++                                ++++++++++++++
# 310 -                 for _ in range(maxlen):
# 346 +                 for _ in tqdm(range(maxlen), desc="token",leave=False):
# 363 +                     else:
# 320 -                     prob = logits.softmax(dim=-1) # shape: (batch_size, num_vocabs)
# 364 +                         prob = logits.softmax(dim=-1) # shape: (batch_size, num_vocabs)
# 364 ? ++++
# 321 -                     now_token = torch.multinomial(prob, 1)[:, :1] # shape: (batch_size)
# 365 +                         now_token = torch.multinomial(prob, 1)[:, :1] # shape: (batch_size, 1)
# 365 ? ++++                                                                                      +++

